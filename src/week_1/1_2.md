# 介绍和理解ANNS(papers)

### ANNS实现思路：
The ANNS methods usually first construct an index
structure to organize data items and then perform a querying search algorithm based on this index to retrieve the
nearest neighbour results for the given queries.

> advantage
>> - Faster search
>> - Don not necessarily have to exact neighbors
>> - Trade off : runtime,accuracy,and memory-consumption
>> - a sense of scale : billion-scale data on memory

根据 Indexing and Searching 的类型分类实现思路:
Hashing-based、Parition-based、Graph-based、Compression-based

| category | Hashing-based[^1]                   | Parition-based[^2]                       | Graph-based[^3]                | Compression-based[^4] |
|----------|-------------------------------------|------------------------------------------|--------------------------------|-----------------------|
|          | LSH (locality sensitive hash-based) | KD-tree                                  | KGraph                         | Faiss                 |
|          |                                     | hierarchical k-means tree                | Small World Graph              | ScaNN                 |
|          |                                     | random projection tree (heuristic-based) | Navigating Spreading-out Graph |                       |

[^1]:have rigorous theoretical guarantees but with poor practical performance in terms of the search accuracy.

[^2]:propose to use heuristic-based strategies to find the data partition,
which cannot take advantages of the data distribution to
produce good partitions for better performance.

[^3]: they mainly focus on the proximity graph
construction with special merits in the degree distribution
and the graph connectivity

[^4]: billion-scale search scenarios; compression-based schemes can often fit necessary
data in the main memory since they only need to hold
the compressed version of the dataset and a lightweight index structure.</br>

### LSH
资料来源：
[LSH的实现原理](https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing/)

代码实操：
[LSH Code](https://github.com/pinecone-io/examples)

论文参考

[1]M. Slaney and M. Casey, "Locality-Sensitive Hashing for Finding Nearest Neighbors [Lecture Notes]," in IEEE Signal Processing Magazine, vol. 25, no. 2, pp. 128-131, March 2008, doi: 10.1109/MSP.2007.914237.

[2]

Process：
* 基于Stable Distribution投影方法</br>
  产生满足Stable Distribution的分布进行投影，最后将量化后的投影值作为value输出。需要同时选择两个参数，并且量化后的哈希值是一个整数而不是bit形式的0和1，你还需要再变换一次。
* 基于随机超平面投影的方法</br>
  提出了一种随机超平面投影LSH. 这种方法的最大优点在于：不需要参数设定;是两个向量间的cosine距离，非常适合于文本度量;计算后的value值是比特形式的1和0，免去了前面算法的再次变化
* SimHash</br>
  前面介绍的LSH算法，都需要首先将样本特征映射为特征向量的形式，使得我们需要额外存储一个映射字典，难免麻烦，大神Charikar又提出了大名鼎鼎的SimHash算法，在满足随机超平面投影LSH特性的同时避免了额外的映射开销，非常适合于token形式的特征。 首先来看SimHash的计算过程： a，将一个f维的向量V初始化为0；f位的二进制数S初始化为0； b，对每一个特征：用传统的hash算法（究竟是哪种算法并不重要，只要均匀就可以）对该特征产生一个f位的签名b。对i=1到f： 如果b的第i位为1，则V的第i个元素加上该特征的权重； 否则，V的第i个元素减去该特征的权重。 c，如果V的第i个元素大于0，则S的第i位为1，否则为0； d，输出S作为签名。

  Pseudocode：

    <img src="../images/w1/simhash.jpg" width = "500" height = "370" alt="图片名称"  />

 
* Kernel LSH



* 传统方法
    k-shinging 和 one-hot编码将文本转换为稀疏向量，然后用最小哈希创建签名，签名被传递给LSH流程以剔除部分候选对
    1. Shingling：把文档转换成集合
    2. Minhashing：把大规模集合转换成短小签名，但是保留相关性
    3. LSH Query：计算可能相似的签名对,调节 M，b，r用相似的签名来得到所有的文档对，但是剔除那些并不相似的签名多数对，检查主要内存，候选对并没有相似签名。


### K-means Clustering

Process：

Pseudocode：

```
获取数据 n 个 m 维的数据
随机生成 K 个 m 维的点
while(t)
    for(int i=0;i < n;i++)
        for(int j=0;j < k;j++)
            计算点 i 到类 j 的距离
    for(int i=0;i < k;i++)
        1. 找出所有属于自己这一类的所有数据点
        2. 把自己的坐标修改为这些数据点的中心点坐标
end
```


### NSW (Navigable Small World Graph)


Process：
1. 随机选择1个元素，放入到candidates当中
2. 从candidates中选取最近邻节点c，将这些元素的邻居节点放置到q当中
3. 从candidates中移除最近邻节点c
4. 如果c的距离远大于result中的第k个节点，跳出循环
5. 否则，对于c的每个邻居节点，遍历其邻居，如果没有在visited set里面。
6. 将e加入到visited set， candidates， tempRes
7. 遍历完成candidate中所有的节点后，把tempRes的结果传入到result
8. 重复执行上述步骤m遍, 返回result中最优的k个近邻结果。

```
K-NNSearch(object q,integer:m,k)
 TreeSet[object]tempRes, candidates, visitedSet, result
  for(i<-0; i< m; i++) do:
   put random entry point in candidates
   tempRes<-null
   repeat:
    get element c closest from candidates to q
    remove c from candidates
   #checks to p condition:
   if c is further than k-th element from result
   than break repeat
   #update list of candidates:
   for every element e from friends of c do:
   if e is not in visited Set than
     add e to visited Set, candidates, tempRes

   end repeat
   #aggregate the results:
   add objects from tempRes to result
  end for
  return best k elements from result

```

### HNSW(Hierarchical Navigable Small World Graph)

资料来源:[NSW & HNSW](https://zhuanlan.zhihu.com/p/264832755)

Process：

1. 在Layer = 0 层中，包含了连通图中所有的点。
2. 随着层数的增加，每一层的点数逐渐减少并且遵循指数衰减定律
3. 图节点的最大层数，由随机指数概率衰减函数决定。
4. 从某个点所在的最高层往下的所有层中均存在该节点。
5. 在对HNSW进行查询的时候，从最高层开始检索。




输入：

输出：插入节点q后的hnsw网络结构

### Product Quantization

资料来源:
[Faiss](https://www.pinecone.io/learn/series/faiss/)
[PQ_BLOG](https://zhuanlan.zhihu.com/p/534004381)

代码实战:[PQ_TEST](https://github.com/xiaopp123/news_recommend_system/blob/main/recall/pq_test/pq.py)

process：
1. Taking a big, high-dimensional vector,
2. Splitting it into equally sized chunks — our subvectors,
3. Assigning each of these subvectors to its nearest centroid (also called reproduction/reconstruction values),
4. Replacing these centroid values with unique IDs — each ID represents a centroid




\\( \int x dx = \frac{x^2}{2} + C \\)













资料来源：

Deep Learning for Approximate Nearest Neighbour Search: A Survey and Future Directions

<img src="../images/w1/2.png" width = "500" height = "550" alt="图片名称" align=center />


